{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2377,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/kunalmehra/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_dir = \"./archive\"\n",
    "file_path = data_dir + \"/data_0.txt\"\n",
    "\n",
    "nltk.data.path.append(data_dir)\n",
    "nltk.download('punkt')\n",
    "\n",
    "with open(file_path, \"r\") as f:\n",
    "    data = f.read()\n",
    "\n",
    "def preprocess_pipeline(data) -> 'list':\n",
    "\n",
    "    sentences = data.split('\\n')\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    sentences = [s for s in sentences if len(s) > 0]\n",
    "    \n",
    "    tokenized = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        \n",
    "        sentence = re.sub(r'([\\(\\)/\"?:!;-])', r' \\g<1> ', sentence)\n",
    "        sentence = re.sub(r'([^0-9]),', r'\\g<1> ,', sentence)\n",
    "        sentence = re.sub(r',([^0-9])', r', \\g<1>', sentence)\n",
    "        sentence = re.sub(r'  +', ' ', sentence)\n",
    "        sentence = sentence.lower()\n",
    "        \n",
    "        token = []\n",
    "\n",
    "        for tok in sentence.split(' '):\n",
    "            if tok == '':\n",
    "                continue\n",
    "            if tok[-1] == '.':\n",
    "                tok = tok[:-1] + ' .'\n",
    "            token.append(tok)\n",
    "\n",
    "        token = ' '.join(token)\n",
    "               \n",
    "        tokens = nltk.word_tokenize(token)\n",
    "        \n",
    "        tokenized.append(tokens)\n",
    "        \n",
    "    return tokenized\n",
    "\n",
    "tokenized_sentences = preprocess_pipeline(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2378,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(tokenized_sentences, random_state=42)\n",
    "\n",
    "train, val = train_test_split(train, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2379,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(sentences) -> 'dict':\n",
    "    \n",
    "  word_counts = {}\n",
    "  for sentence in sentences:\n",
    "    \n",
    "    for token in sentence:\n",
    "    \n",
    "      if token not in word_counts.keys():\n",
    "        word_counts[token] = 1\n",
    "        \n",
    "      else:\n",
    "        word_counts[token] += 1\n",
    "        \n",
    "  return word_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2380,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_OOVocab(tokenized_sentences, count_threshold) -> 'list':\n",
    "\n",
    "  closed_vocabulary = []\n",
    "\n",
    "  words_count = count_words(tokenized_sentences)\n",
    "    \n",
    "  for word, count in words_count.items():\n",
    "    \n",
    "    if count >= count_threshold :\n",
    "      closed_vocabulary.append(word)\n",
    "\n",
    "  return closed_vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2381,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_threshold = 1\n",
    "\n",
    "#Closed Vocabulary\n",
    "vocab = handle_OOVocab(train, count_threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2382,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unk_tokenize(tokenized_sentences, vocabulary, unknown_token = \"<unk>\") -> 'list':\n",
    "\n",
    "  vocabulary = set(vocabulary)\n",
    "\n",
    "  new_tokenized_sentences = []\n",
    "  \n",
    "  for sentence in tokenized_sentences:\n",
    "\n",
    "    new_sentence = []\n",
    "    for token in sentence:\n",
    "      if token in vocabulary:\n",
    "        new_sentence.append(token)\n",
    "      else:\n",
    "        new_sentence.append(unknown_token)\n",
    "    \n",
    "    new_tokenized_sentences.append(new_sentence)\n",
    "\n",
    "  return new_tokenized_sentences\n",
    "\n",
    "\n",
    "final_train = unk_tokenize(train, vocab)\n",
    "    \n",
    "final_test = unk_tokenize(test, vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2383,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to map n-grams to their respective frequencies in the dataset\n",
    "\n",
    "def count_n_grams(data, n, start_token = \"<s>\", end_token = \"<e>\") -> 'dict':\n",
    "\n",
    "  n_grams = {}\n",
    " \n",
    "  for sentence in data:\n",
    "        \n",
    "    sentence = [start_token]*n + sentence + [end_token]\n",
    "    \n",
    "    sentence = tuple(sentence)\n",
    "\n",
    "    if n==1:\n",
    "        m = len(sentence)\n",
    "    else:\n",
    "        m = len(sentence)-1\n",
    "    \n",
    "    for i in range(m):\n",
    "        \n",
    "      n_gram = sentence[i:i+n]\n",
    "    \n",
    "      if n_gram in n_grams.keys():\n",
    "        n_grams[n_gram] += 1\n",
    "\n",
    "      else:\n",
    "        n_grams[n_gram] = 1\n",
    "        \n",
    "  return n_grams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2384,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_prob(word, previous_n_gram, n_gram_counts, nplus1_gram_counts, vocabulary_size) -> 'float':\n",
    "\n",
    "  # Smoothing factor\n",
    "  k = 1\n",
    "\n",
    "  previous_n_gram = tuple(previous_n_gram)\n",
    "    \n",
    "  previous_n_gram_count = n_gram_counts[previous_n_gram] if previous_n_gram in n_gram_counts else 0\n",
    "  \n",
    "  den = previous_n_gram_count + k * vocabulary_size\n",
    "\n",
    "  nplus1_gram = previous_n_gram + (word,)\n",
    "\n",
    "  nplus1_gram_count = nplus1_gram_counts[nplus1_gram] if nplus1_gram in nplus1_gram_counts else 0\n",
    "\n",
    "  num = nplus1_gram_count + k\n",
    "\n",
    "  # Final Fraction\n",
    "  prob = num / den\n",
    "\n",
    "  return prob\n",
    "\n",
    "\n",
    "\n",
    "def get_probs(previous_n_gram, n_gram_counts, nplus1_gram_counts, vocabulary) -> 'dict':\n",
    "\n",
    "  previous_n_gram = tuple(previous_n_gram)\n",
    "\n",
    "  vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\n",
    "\n",
    "  vocabulary_size = len(vocabulary)\n",
    "\n",
    "  probabilities = {}\n",
    "\n",
    "  for word in vocabulary:\n",
    "    \n",
    "    probability = get_prob(word, previous_n_gram,n_gram_counts, nplus1_gram_counts,vocabulary_size)\n",
    "\n",
    "    probabilities[word] = probability\n",
    "\n",
    "  return probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.', 0.03466204506065858), ('.', 0.007366482504604052), ('.', 0.007366482504604052), ('jab', 0.0018552875695732839)]\n"
     ]
    }
   ],
   "source": [
    "def get_suggestion(previous_tokens, n_gram_counts, nplus1_gram_counts, vocabulary, start_with=None):\n",
    "\n",
    "    n = len(list(n_gram_counts.keys())[0]) \n",
    "    \n",
    "    # most recent 'n' words\n",
    "    previous_n_gram = previous_tokens[-n:]\n",
    "    \n",
    "    probabilities = get_probs(previous_n_gram,n_gram_counts, nplus1_gram_counts,vocabulary)\n",
    "\n",
    "    suggestion = None\n",
    "    max_prob = 0\n",
    "\n",
    "    for word, prob in probabilities.items():\n",
    "        \n",
    "        if start_with != None: \n",
    "            \n",
    "            if not word.startswith(start_with):\n",
    "                continue \n",
    "\n",
    "        if prob > max_prob: \n",
    "\n",
    "            suggestion = word\n",
    "            max_prob = prob\n",
    "\n",
    "    return suggestion, max_prob\n",
    "\n",
    "\n",
    "def get_next_word(previous_tokens, n_gram_counts_list, vocabulary, start_with=None):\n",
    "\n",
    "    count = len(n_gram_counts_list)\n",
    "    \n",
    "    suggestions = []\n",
    "    \n",
    "    for i in range(count-1):\n",
    "        \n",
    "        # get n and nplus1 counts\n",
    "        n_gram_counts = n_gram_counts_list[i]\n",
    "        nplus1_gram_counts = n_gram_counts_list[i+1]\n",
    "        \n",
    "        suggestion = get_suggestion(previous_tokens, n_gram_counts, nplus1_gram_counts, vocabulary, start_with=start_with)\n",
    "\n",
    "        suggestions.append(suggestion)\n",
    "        \n",
    "    return (suggestions)\n",
    "\n",
    "\n",
    "previous_tokens = ['kar','sakte','hai']\n",
    "\n",
    "\n",
    "n_gram_counts_list = []\n",
    "for n in range(1, 6):\n",
    "    n_model_counts = count_n_grams(final_train, n)\n",
    "    n_gram_counts_list.append(n_model_counts)\n",
    "\n",
    "# print(n_gram_counts_list)\n",
    "\n",
    "suggestion = get_next_word(previous_tokens, n_gram_counts_list, vocab)\n",
    "\n",
    "print(suggestion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8bffc19cc6ddf61a36324c1cf842c6619adda1dd6e5bae6d1a0be3294bdfcb4c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
